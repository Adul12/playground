{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3: 5x5 Gridworld\n",
    "\n",
    "#### Changes in Value Iteration Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld\n",
    "from value_iteration_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, theta_threshold=0.01):\n",
    "        # Initialize the agent with the environment and parameters\n",
    "        self.env_size = env.get_size()\n",
    "        self.env = env\n",
    "        # Value function initialization\n",
    "        self.V = np.zeros((self.env_size, self.env_size))  \n",
    "        # Define the terminal state\n",
    "        self.terminal_state = (4, 4)\n",
    "        # Value of the terminal state is zero  \n",
    "        self.V[self.terminal_state] = 0  \n",
    "        # Threshold for convergence\n",
    "        self.theta_threshold = theta_threshold \n",
    "        # Get the list of possible actions \n",
    "        self.actions = env.get_actions() \n",
    "        # Discount factor \n",
    "        self.gamma = 1.0  \n",
    "        # Initialize policy\n",
    "        self.pi_greedy = np.zeros((self.env_size, self.env_size), dtype=int)  \n",
    "\n",
    "    def calculate_max_value(self, i, j):\n",
    "        # Calculate the maximum value and best action for state (i, j)\n",
    "        max_value = float('-inf')\n",
    "        best_action = None\n",
    "        best_actions_str = \"\"\n",
    "        for action_index in range(len(self.actions)):\n",
    "            next_i, next_j, reward, _ = self.env.step(action_index, i, j)\n",
    "            if self.env.is_valid_state(next_i, next_j):\n",
    "                value = self.get_value(next_i, next_j, reward)\n",
    "                if value >= max_value:\n",
    "                    if value > max_value:\n",
    "                        best_actions_str = self.env.action_description[action_index]\n",
    "                    else:\n",
    "                        best_actions_str += \"|\" + self.env.action_description[action_index]\n",
    "\n",
    "                    best_action = action_index\n",
    "                    max_value = value\n",
    "        return max_value, best_action, best_actions_str\n",
    "\n",
    "    def get_value(self, i, j, reward):\n",
    "        # Calculate the value of a state given a reward\n",
    "        return reward + self.gamma * self.V[i, j]\n",
    "\n",
    "    def update_value_function(self, V):\n",
    "        # Update the value function\n",
    "        self.V = np.copy(V)\n",
    "\n",
    "    def get_value_function(self):\n",
    "        # Return the current value function\n",
    "        return self.V\n",
    "\n",
    "    def update_greedy_policy(self):\n",
    "        # Update the policy to be greedy with respect to the value function\n",
    "        self.pi_str = []\n",
    "        for i in range(self.env_size):\n",
    "            pi_row = []\n",
    "            for j in range(self.env_size):\n",
    "                if self.env.is_terminal_state(i,j):\n",
    "                    pi_row.append(\"X\")  # Mark terminal state in the policy\n",
    "                    continue\n",
    "                _, self.pi_greedy[i,j], action_str = self.calculate_max_value(i, j)\n",
    "                pi_row.append(action_str)\n",
    "            self.pi_str.append(pi_row)\n",
    "\n",
    "    def is_done(self, new_V):\n",
    "        # Check if the value function has converged\n",
    "        delta = abs(self.V - new_V)\n",
    "        max_delta = delta.max()\n",
    "        return max_delta <= self.theta_threshold\n",
    "\n",
    "    def get_policy(self):\n",
    "        # Return the greedy policy\n",
    "        return self.pi_greedy\n",
    "\n",
    "    def print_policy(self):\n",
    "        # Print the policy\n",
    "        for row in self.pi_str:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes in Value Iteration Solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function Found in 9 iterations:\n",
      "[[-7. -6. -5. -4. -3.]\n",
      " [-6. -5. -4. -3. -2.]\n",
      " [-5. -4. -3. -2. -1.]\n",
      " [-4. -3. -2. -1.  0.]\n",
      " [-3. -2. -1.  0.  0.]]\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ENV_SIZE = 5\n",
    "    THETA_THRESHOLD = 0.05\n",
    "    MAX_ITERATIONS = 1000\n",
    "    # Initialize the environment and agent\n",
    "    env = GridWorld(ENV_SIZE)\n",
    "    agent = Agent(env, THETA_THRESHOLD)\n",
    "    # Perform value iteration\n",
    "    done = False\n",
    "    for iter in range(MAX_ITERATIONS):\n",
    "        if done: \n",
    "            break\n",
    "        # Copy the current value function to new_V\n",
    "        new_V = np.copy(agent.get_value_function())\n",
    "        for i in range(ENV_SIZE):\n",
    "            for j in range(ENV_SIZE):\n",
    "                if not env.is_terminal_state(i, j):\n",
    "                    # Calculate the maximum value for each state\n",
    "                    new_V[i, j], _, _ = agent.calculate_max_value(i, j)\n",
    "        # Check if the value function has converged\n",
    "        done = agent.is_done(new_V)\n",
    "        # Update the agent's value function\n",
    "        agent.update_value_function(new_V)\n",
    "    # optimal value function\n",
    "    print(\"Optimal Value Function Found in %d iterations:\" % (iter + 1))\n",
    "    print(agent.get_value_function())\n",
    "    # Update and print the greedy policy\n",
    "    agent.update_greedy_policy()\n",
    "    agent.print_policy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Iteration Agent**\n",
    "*Updated and implemented methods for calculating the maximum value, checking convergence and updating the policy.*\n",
    "\n",
    "**Value Iteration Solved**\n",
    "*Main function to perform value iterartion using the agent in th place updates and convergence check.*\n",
    "\n",
    "\n",
    "*Hence, these updtes implement the value iteration algorithm, including in-place updates and policy improvement to find the optimal state-value function and its policy for Grid-World environment.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
